{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Similarity Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need to run this code first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Natural language processing is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language.\"\n",
    "\n",
    "sentence_collection = [\n",
    "    'Natural language processing is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language.',\n",
    "\n",
    "    'Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.'\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`sent_tokenize` is used to tokenize a text into sentences**\n",
    "- It accepts only a single string at a time. It does not accept a collection of string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language processing is an interdisciplinary subfield of computer science and linguistics.',\n",
       " 'It is primarily concerned with giving computers the ability to support and manipulate human language.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing is an interdisciplinary subfield of computer science and linguistics.', 'It is primarily concerned with giving computers the ability to support and manipulate human language.']\n",
      "['Natural language processing has its roots in the 1950s.', 'Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence.', 'The proposed test includes a task that involves the automated interpretation and generation of natural language.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentence_collection:\n",
    "    print(sent_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`word_tokenize` is used to tokenize a sentence or a piece of text into words.**\n",
    "- It accepts only a single string at a time. It does not accept a collection of string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs1 = [\n",
    "    \"Harry Potter # and the Sorcerer's Stone\",\n",
    "    \"Harry Potter #2 and the Chamber of Secrets\",\n",
    "    \"The Sorcerer's Den! 5s\",\n",
    "    \"Great! Sorcerer's of NY 2\",\n",
    "    \"Great Secrets of Amazon\",\n",
    "    \"S\",\n",
    "    \"Ss\",\n",
    "    \"7x7\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry', 'Potter', '#', 'and', 'the', 'Sorcerer', \"'s\", 'Stone']\n",
      "['Harry', 'Potter', '#', '2', 'and', 'the', 'Chamber', 'of', 'Secrets']\n",
      "['The', 'Sorcerer', \"'s\", 'Den', '!', '5s']\n",
      "['Great', '!', 'Sorcerer', \"'s\", 'of', 'NY', '2']\n",
      "['Great', 'Secrets', 'of', 'Amazon']\n",
      "['S']\n",
      "['Ss']\n",
      "['7x7']\n"
     ]
    }
   ],
   "source": [
    "for doc in docs1:\n",
    "    print(word_tokenize(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structured way of creating tokenized list of documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['harry', 'potter', '#', 'and', 'the', 'sorcerer', \"'s\", 'stone'],\n",
       " ['harry', 'potter', '#', '2', 'and', 'the', 'chamber', 'of', 'secrets'],\n",
       " ['the', 'sorcerer', \"'s\", 'den', '!', '5s'],\n",
       " ['great', '!', 'sorcerer', \"'s\", 'of', 'ny', '2'],\n",
       " ['great', 'secrets', 'of', 'amazon'],\n",
       " ['s'],\n",
       " ['ss'],\n",
       " ['7x7']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_documents = [word_tokenize(document.lower()) for document in docs1]\n",
    "tokenized_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is a smart word tokenizer which understand sentences properly and can differentiate between words and punctuations and also consider them as valid tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(tokenized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `BM25Okapi` takes only tokenized collection of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.15792005, 2.68086623, 0.        , 0.77615639, 0.        ,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Harry Potter #2\"\n",
    "\n",
    "tokenized_query = word_tokenize(query.lower())\n",
    "\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We get the scores as 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harry Potter #2 and the Chamber of Secrets',\n",
       " \"Harry Potter # and the Sorcerer's Stone\",\n",
       " \"Great! Sorcerer's of NY 2\",\n",
       " '7x7']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Harry Potter #2\"\n",
    "tokenized_query = word_tokenize(query.lower())\n",
    "\n",
    "top_n = bm25.get_top_n(tokenized_query, docs1, n=4)\n",
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can get the matches directly based on the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['harry', 'potter', '#', '2', 'and', 'the', 'chamber', 'of', 'secrets'],\n",
       " ['harry', 'potter', '#', 'and', 'the', 'sorcerer', \"'s\", 'stone'],\n",
       " ['great', '!', 'sorcerer', \"'s\", 'of', 'ny', '2'],\n",
       " ['7x7']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Harry Potter #2\"\n",
    "tokenized_query = word_tokenize(query.lower())\n",
    "\n",
    "top_n = bm25.get_top_n(tokenized_query, tokenized_documents, n=4)\n",
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can get the matches based on the scores in the tokenized form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop code execution\n",
    "\n",
    "10/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TFIDF + Cosine Similarity) vs BM25 - Output Check and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(docs1,query_str):\n",
    "    tf1 = TfidfVectorizer(analyzer='word', ngram_range=(1, 1),\n",
    "                     min_df=0)\n",
    "    tfidf1 = tf1.fit_transform(docs1)\n",
    "    cosine_sim1 = cosine_similarity(tfidf1, tf1.transform([query_str]))\n",
    "\n",
    "    return cosine_sim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(docs1,query_str):\n",
    "    tokenized_query = word_tokenize(query_str.lower())\n",
    "    tokenized_documents = [word_tokenize(document.lower()) for document in docs1]\n",
    "    bm25 = BM25Okapi(tokenized_documents)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs1 = [\n",
    "    \"Harry Potter # and the Sorcerer's Stone\",\n",
    "    \"Harry Potter #2 and the Chamber of Secrets\",\n",
    "    \"The Sorcerer's Den! 5s\",\n",
    "    \"Great! Sorcerer's of NY 2\",\n",
    "    \"Great Secrets of Amazon\",\n",
    "    \"S\",\n",
    "    \"Ss\",\n",
    "    \"7x7\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 7, 6]\n",
      "[0, 1, 7, 6]\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Harry Potter\"\n",
    "\n",
    "print(list(reversed((np.argsort(tfidf(docs1,query_str).flatten())[-4:]))))\n",
    "print(list(reversed((np.argsort(bm25(docs1,query_str))[-4:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 2, 7]\n",
      "[3, 0, 2, 7]\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Sorcerer's Stone NY\"\n",
    "\n",
    "print(list(reversed((np.argsort(tfidf(docs1,query_str).flatten())[-4:]))))\n",
    "print(list(reversed((np.argsort(bm25(docs1,query_str))[-4:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 7]\n",
      "[0, 1, 2, 7]\n"
     ]
    }
   ],
   "source": [
    "query_str = \"the Potter Harry \"\n",
    "\n",
    "print(list(reversed((np.argsort(tfidf(docs1,query_str).flatten())[-4:]))))\n",
    "print(list(reversed((np.argsort(bm25(docs1,query_str))[-4:]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For exact token matches performance accuracy is same at this scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 3, 2]\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Harry Potter Sorcerer's\"\n",
    "\n",
    "print(list(reversed((np.argsort(tfidf(docs1,query_str).flatten())[-4:]))))\n",
    "print(list(reversed((np.argsort(bm25(docs1,query_str))[-4:]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see a difference here because `Sorcerer's` gets tokenized differently in two modules.\n",
    "  - In BM25, `Sorcerer's` => `sorcerer`, `'s`\n",
    "  - In TFIDF, `Sorcerer's` => `sorcerer` but there won't be any token as `'s`\n",
    "- So, there is a difference in tokens and so in score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine `nltk` for Vocabulary Generation and (`tfidf` + `Cosine Similarity`) for Similarity Calculation Vs BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs1 = [\n",
    "    \"Harry Potter # and the Sorcerer's Stone\",\n",
    "    \"Harry Potter #2 and the Chamber of Secrets\",\n",
    "    \"The Sorcerer's Den! 5s\",\n",
    "    \"Great! Sorcerer's of NY 2\",\n",
    "    \"Great Secrets of Amazon\",\n",
    "    \"S\",\n",
    "    \"Ss\",\n",
    "    \"7x7\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amazon', 'great', '!', 'of', '#', 'den', 'secrets', '7x7', 's', \"'s\", 'the', 'ny', 'and', 'potter', 'stone', 'ss', 'harry', 'sorcerer', '5s', '2', 'chamber'}\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "vocabulary_set = set()\n",
    "\n",
    "for document in docs1:\n",
    "    vocabulary_set.update(word_tokenize(document.lower()))\n",
    "\n",
    "print(vocabulary_set)\n",
    "print(len(vocabulary_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting the list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '#', \"'s\", '2', '5s', '7x7', 'amazon', 'and', 'chamber', 'den', 'great', 'harry', 'ny', 'of', 'potter', 's', 'secrets', 'sorcerer', 'ss', 'stone', 'the']\n"
     ]
    }
   ],
   "source": [
    "vocabulary_list = list(vocabulary_set)\n",
    "vocabulary_list.sort()\n",
    "print(vocabulary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 21)\n",
      "==================================================\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.41124081 0.         0.         0.         0.41124081\n",
      "  0.         0.         0.41124081 0.         0.         0.35486708\n",
      "  0.         0.49069512 0.35486708]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.38033548 0.45381869 0.         0.         0.38033548\n",
      "  0.         0.32819832 0.38033548 0.         0.38033548 0.\n",
      "  0.         0.         0.32819832]\n",
      " [0.         0.         0.         0.         0.57297276 0.\n",
      "  0.         0.         0.         0.57297276 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.41436966\n",
      "  0.         0.         0.41436966]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.50552809 0.\n",
      "  0.6031993  0.43622927 0.         0.         0.         0.43622927\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.58442997 0.         0.         0.         0.48979792 0.\n",
      "  0.         0.42265542 0.         0.         0.48979792 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tf1 = TfidfVectorizer(analyzer='word', ngram_range=(1, 1),\n",
    "                    min_df=0, vocabulary=vocabulary_set)\n",
    "\n",
    "tfidf1 = tf1.fit_transform(docs1)\n",
    "\n",
    "print(tfidf1.toarray().shape)\n",
    "print(\"=\"*50)\n",
    "print(tfidf1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can observe that `'!', '#', \"'s\", '2', 's'` not getting considered as any match\n",
    "- `\"S\"` not getting considered as any valid vector eventhough there is a term `'s'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!', '#', \"'s\", '2', '5s', '7x7', 'amazon', 'and', 'chamber',\n",
       "       'den', 'great', 'harry', 'ny', 'of', 'potter', 's', 'secrets',\n",
       "       'sorcerer', 'ss', 'stone', 'the'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf1.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.64485945,\n",
       "        0.        , 0.54044255, 0.        , 0.        , 0.54044255,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_str = \"Harry Potter Den!\"\n",
    "tf1.transform([query_str]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can clearly observe that there is a tfidf score for the terms `Harry`, `Potter`, `den` but there is no score for `!` although it is present in the Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docs1 = [\n",
    "    \"Harry Potter # and the Sorcerer's Stone\",\n",
    "    \"Harry Potter #2 and the Chamber of Secrets\",\n",
    "    \"The Sorcerer's Den! 5s\",\n",
    "    \"Great! Sorcerer's of NY 2\",\n",
    "    \"Great Secrets of Amazon\",\n",
    "    \"S\",\n",
    "    \"Ss\",\n",
    "    \"7x7\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44450406],\n",
       "       [0.41109895],\n",
       "       [0.3694869 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cosine_sim1 = cosine_similarity(tfidf1, tf1.transform([query_str]))\n",
    "\n",
    "cosine_sim1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can observe that `Harry Potter Den!` has similarities in descending order as follows.\n",
    "\n",
    "```\n",
    "    \"Harry Potter # and the Sorcerer's Stone\",\n",
    "    \"Harry Potter #2 and the Chamber of Secrets\",\n",
    "    \"The Sorcerer's Den! 5s\",\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.43861337, 1.34043312, 2.26229145, 0.77615639, 0.        ,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_str = \"Harry Potter Den!\"\n",
    "\n",
    "tokenized_query = word_tokenize(query_str.lower())\n",
    "\n",
    "tokenized_documents = [word_tokenize(document.lower()) for document in docs1]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_documents)\n",
    "\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can observe that `Harry Potter Den!` has similarities in descending order as follows.\n",
    "\n",
    "```\n",
    "    \"The Sorcerer's Den! 5s\",\n",
    "    \"Harry Potter # and the Sorcerer's Stone\",\n",
    "    \"Harry Potter #2 and the Chamber of Secrets\",\n",
    "    \"Great! Sorcerer's of NY 2\",\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'sorcerer', \"'s\", 'den', '!', '5s'],\n",
       " ['harry', 'potter', '#', 'and', 'the', 'sorcerer', \"'s\", 'stone'],\n",
       " ['harry', 'potter', '#', '2', 'and', 'the', 'chamber', 'of', 'secrets'],\n",
       " ['great', '!', 'sorcerer', \"'s\", 'of', 'ny', '2']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_str = \"Harry Potter Den!\"\n",
    "\n",
    "tokenized_query = word_tokenize(query_str.lower())\n",
    "\n",
    "tokenized_documents = [word_tokenize(document.lower()) for document in docs1]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_documents)\n",
    "\n",
    "top_n = bm25.get_top_n(tokenized_query, tokenized_documents, n=4)\n",
    "\n",
    "top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Sorcerer's Den! 5s\",\n",
       " \"Harry Potter # and the Sorcerer's Stone\",\n",
       " 'Harry Potter #2 and the Chamber of Secrets',\n",
       " \"Great! Sorcerer's of NY 2\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_str = \"Harry Potter Den!\"\n",
    "\n",
    "tokenized_query = word_tokenize(query_str.lower())\n",
    "\n",
    "tokenized_documents = [word_tokenize(document.lower()) for document in docs1]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_documents)\n",
    "\n",
    "top_n = bm25.get_top_n(tokenized_query, docs1, n=4)\n",
    "\n",
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the tfidfVectorizer's default tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 16)\n"
     ]
    }
   ],
   "source": [
    "tf1 = TfidfVectorizer(analyzer='word', ngram_range=(1, 1),\n",
    "                    min_df=0)\n",
    "\n",
    "tfidf1 = tf1.fit_transform(docs1)\n",
    "\n",
    "print(tfidf1.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5s', '7x7', 'amazon', 'and', 'chamber', 'den', 'great', 'harry',\n",
       "       'ny', 'of', 'potter', 'secrets', 'sorcerer', 'ss', 'stone', 'the'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf1.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.64485945, 0.        , 0.54044255, 0.        , 0.        ,\n",
       "        0.54044255, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_str = \"Harry Potter Den!\"\n",
    "tf1.transform([query_str]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docs1 = [\n",
    "    \"Harry Potter # and the Sorcerer's Stone\",\n",
    "    \"Harry Potter #2 and the Chamber of Secrets\",\n",
    "    \"The Sorcerer's Den! 5s\",\n",
    "    \"Great! Sorcerer's of NY 2\",\n",
    "    \"Great Secrets of Amazon\",\n",
    "    \"S\",\n",
    "    \"Ss\",\n",
    "    \"7x7\"\n",
    "]\n",
    "```\n",
    "\n",
    "```\n",
    "array([[0.44450406],\n",
    "       [0.41109895],\n",
    "       [0.3694869 ],\n",
    "       [0.        ],\n",
    "       [0.        ],\n",
    "       [0.        ],\n",
    "       [0.        ],\n",
    "       [0.        ]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44450406],\n",
       "       [0.41109895],\n",
       "       [0.3694869 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cosine_sim1 = cosine_similarity(tfidf1, tf1.transform([query_str]))\n",
    "\n",
    "cosine_sim1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can observe that the score is same as the above tfidfVectorizer and Cosine Similary pair\n",
    "- We can conclude that, in tfidfVectorizer, punctuations are not considered as terms or vocabulary even if it is present in the vocabularity of the tfidfVectorizer object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
