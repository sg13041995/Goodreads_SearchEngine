{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing the data: This involves cleaning and preprocessing the data to get it ready for indexing and searching. This can include tasks such as tokenization, stemming, and removing stop words.\n",
    "2. Tokenization: A dictionary is a mapping between words and their integer IDs. You can use Gensim’s Dictionary class to create a dictionary from a list of tokenized documents.\n",
    "3. Creating a corpus: A corpus is a collection of documents represented as a list of vectors, where each vector corresponds to a document and consists of the integer IDs of the words in that document. You can use the doc2bow method of the Dictionary class to create a corpus from a list of tokenized documents.\n",
    "4. Indexing the corpus: To build a search engine, you need to index the corpus so that you can efficiently search for documents that contain a given set of words. You can use Gensim’s TfidfModel class to transform the corpus into a form that can be easily searched using the BM25 ranking function.\n",
    "5. Searching the index: Once the index is created, you can use the BM25 class to search the index for documents that match a given query. The BM25 class takes a query and the index as input and returns a list of documents ranked according to their BM25 scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, OkapiBM25Model\n",
    "from gensim.similarities import SparseMatrixSimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using sample data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [\n",
    "    \"Eternal Reverie\",\n",
    "    \"Whispers of Time\",\n",
    "    \"Shadows in the Sky\",\n",
    "    \"Infinite Echoes\",\n",
    "    \"Emerald Serenade\",\n",
    "    \"Mystic Horizon\",\n",
    "    \"Celestial Rhapsody\",\n",
    "    \"Silent Symphony\",\n",
    "    \"Aurora's Embrace\",\n",
    "    \"Chronicles of Destiny\",\n",
    "    \"Harry Potter and the Philosopher's Stone (Sorcerer's Stone) - 2001\",\n",
    "    \"Harry Potter and the Chamber of Secrets - 2002\",\n",
    "    \"Harry Potter and the Prisoner of Azkaban - 2004\",\n",
    "    \"Harry Potter and the Goblet of Fire - 2005\",\n",
    "    \"Harry Potter and the Order of the Phoenix - 2007\",\n",
    "    \"Harry Potter and the Half-Blood Prince - 2009\",\n",
    "    \"Harry Potter and the Deathly Hallows – Part 1 - 2010\",\n",
    "    \"Harry Potter and the Deathly Hallows – Part 2 - 2011\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for initial cleaning of the string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(string):\n",
    "    return re.sub(\"\\s+\", \" \", re.sub(\"[^a-zA-Z0-9 ]\", \"\", string.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating the tokenized corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['eternal', 'reverie'],\n",
       " ['whispers', 'of', 'time'],\n",
       " ['shadows', 'in', 'the', 'sky'],\n",
       " ['infinite', 'echoes'],\n",
       " ['emerald', 'serenade'],\n",
       " ['mystic', 'horizon'],\n",
       " ['celestial', 'rhapsody'],\n",
       " ['silent', 'symphony'],\n",
       " ['auroras', 'embrace'],\n",
       " ['chronicles', 'of', 'destiny'],\n",
       " ['harry',\n",
       "  'potter',\n",
       "  'and',\n",
       "  'the',\n",
       "  'philosophers',\n",
       "  'stone',\n",
       "  'sorcerers',\n",
       "  'stone',\n",
       "  '2001'],\n",
       " ['harry', 'potter', 'and', 'the', 'chamber', 'of', 'secrets', '2002'],\n",
       " ['harry', 'potter', 'and', 'the', 'prisoner', 'of', 'azkaban', '2004'],\n",
       " ['harry', 'potter', 'and', 'the', 'goblet', 'of', 'fire', '2005'],\n",
       " ['harry', 'potter', 'and', 'the', 'order', 'of', 'the', 'phoenix', '2007'],\n",
       " ['harry', 'potter', 'and', 'the', 'halfblood', 'prince', '2009'],\n",
       " ['harry', 'potter', 'and', 'the', 'deathly', 'hallows', 'part', '1', '2010'],\n",
       " ['harry', 'potter', 'and', 'the', 'deathly', 'hallows', 'part', '2', '2011']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for doc in doc_list:\n",
    "    clean_str = clean_string(doc)\n",
    "    corpus.append(word_tokenize(clean_str))\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary is a mapping between words and their integer IDs**\n",
    "- In Gensim, the Dictionary class is used to create and manage a mapping between words (or tokens) and their integer identifiers. It is a fundamental component in the process of converting text documents into numerical representations that can be used in various natural language processing (NLP) and machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.corpora.dictionary.Dictionary'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'eternal', 1: 'reverie', 2: 'of', 3: 'time', 4: 'whispers', 5: 'in', 6: 'shadows', 7: 'sky', 8: 'the', 9: 'echoes', 10: 'infinite', 11: 'emerald', 12: 'serenade', 13: 'horizon', 14: 'mystic', 15: 'celestial', 16: 'rhapsody', 17: 'silent', 18: 'symphony', 19: 'auroras', 20: 'embrace', 21: 'chronicles', 22: 'destiny', 23: '2001', 24: 'and', 25: 'harry', 26: 'philosophers', 27: 'potter', 28: 'sorcerers', 29: 'stone', 30: '2002', 31: 'chamber', 32: 'secrets', 33: '2004', 34: 'azkaban', 35: 'prisoner', 36: '2005', 37: 'fire', 38: 'goblet', 39: '2007', 40: 'order', 41: 'phoenix', 42: '2009', 43: 'halfblood', 44: 'prince', 45: '1', 46: '2010', 47: 'deathly', 48: 'hallows', 49: 'part', 50: '2', 51: '2011'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eternal': 0,\n",
       " 'reverie': 1,\n",
       " 'of': 2,\n",
       " 'time': 3,\n",
       " 'whispers': 4,\n",
       " 'in': 5,\n",
       " 'shadows': 6,\n",
       " 'sky': 7,\n",
       " 'the': 8,\n",
       " 'echoes': 9,\n",
       " 'infinite': 10,\n",
       " 'emerald': 11,\n",
       " 'serenade': 12,\n",
       " 'horizon': 13,\n",
       " 'mystic': 14,\n",
       " 'celestial': 15,\n",
       " 'rhapsody': 16,\n",
       " 'silent': 17,\n",
       " 'symphony': 18,\n",
       " 'auroras': 19,\n",
       " 'embrace': 20,\n",
       " 'chronicles': 21,\n",
       " 'destiny': 22,\n",
       " '2001': 23,\n",
       " 'and': 24,\n",
       " 'harry': 25,\n",
       " 'philosophers': 26,\n",
       " 'potter': 27,\n",
       " 'sorcerers': 28,\n",
       " 'stone': 29,\n",
       " '2002': 30,\n",
       " 'chamber': 31,\n",
       " 'secrets': 32,\n",
       " '2004': 33,\n",
       " 'azkaban': 34,\n",
       " 'prisoner': 35,\n",
       " '2005': 36,\n",
       " 'fire': 37,\n",
       " 'goblet': 38,\n",
       " '2007': 39,\n",
       " 'order': 40,\n",
       " 'phoenix': 41,\n",
       " '2009': 42,\n",
       " 'halfblood': 43,\n",
       " 'prince': 44,\n",
       " '1': 45,\n",
       " '2010': 46,\n",
       " 'deathly': 47,\n",
       " 'hallows': 48,\n",
       " 'part': 49,\n",
       " '2': 50,\n",
       " '2011': 51}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the data into the `OkapiBM25Model`**\n",
    "- The OkapiBM25Model class in Gensim is used to apply the Okapi BM25 ranking function to a corpus of documents. Okapi BM25 is a ranking function commonly used for information retrieval and text mining. It is an extension of the term frequency-inverse document frequency (TF-IDF) model and is designed to address some of its limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_model = OkapiBM25Model(dictionary=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating bag-of-words (BoW) representation of documents and feeding that into `OkapiBM25Model` object**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of words representation of corpus**\n",
    "- In the context of Gensim, the doc2bow function is used in the process of creating a bag-of-words (BoW) representation of a document. This is a common step in natural language processing (NLP) and information retrieval tasks.\n",
    "\n",
    "```\n",
    "[\n",
    "  [(0, 1), (1, 1)],\n",
    "  [(2, 1), (3, 1), (4, 1)],\n",
    "  [(5, 1), (6, 1), (7, 1), (8, 1)]\n",
    "  ...\n",
    "]\n",
    "```  \n",
    "  - (word_id, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(11, 1), (12, 1)],\n",
       " [(13, 1), (14, 1)],\n",
       " [(15, 1), (16, 1)],\n",
       " [(17, 1), (18, 1)],\n",
       " [(19, 1), (20, 1)],\n",
       " [(2, 1), (21, 1), (22, 1)],\n",
       " [(8, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 2)],\n",
       " [(2, 1), (8, 1), (24, 1), (25, 1), (27, 1), (30, 1), (31, 1), (32, 1)],\n",
       " [(2, 1), (8, 1), (24, 1), (25, 1), (27, 1), (33, 1), (34, 1), (35, 1)],\n",
       " [(2, 1), (8, 1), (24, 1), (25, 1), (27, 1), (36, 1), (37, 1), (38, 1)],\n",
       " [(2, 1), (8, 2), (24, 1), (25, 1), (27, 1), (39, 1), (40, 1), (41, 1)],\n",
       " [(8, 1), (24, 1), (25, 1), (27, 1), (42, 1), (43, 1), (44, 1)],\n",
       " [(8, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (27, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1)],\n",
       " [(8, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (27, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 1),\n",
       "  (51, 1)]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(dictionary.doc2bow, corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`bm25_model` is scoring the corpus**\n",
    "\n",
    "```\n",
    "[\n",
    "  [(0, 3.374535174743225), (1, 3.374535174743225)],\n",
    "  [(2, 0.8003672970276591), (3, 3.0068991974006543), (4, 3.0068991974006543)],\n",
    "  [(5, 2.7114973356790624), (6, 2.7114973356790624), (7, 2.7114973356790624), (8, 0.0)],\n",
    "  ...\n",
    "]\n",
    "```  \n",
    "  - (word_id, some sort of score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_corpus = bm25_model[list(map(dictionary.doc2bow, corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.interfaces.TransformedCorpus'>\n"
     ]
    }
   ],
   "source": [
    "print(type(bm25_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 3.374535174743225), (1, 3.374535174743225)],\n",
       " [(2, 0.8003672970276591), (3, 3.0068991974006543), (4, 3.0068991974006543)],\n",
       " [(5, 2.7114973356790624),\n",
       "  (6, 2.7114973356790624),\n",
       "  (7, 2.7114973356790624),\n",
       "  (8, 0.0)],\n",
       " [(9, 3.374535174743225), (10, 3.374535174743225)],\n",
       " [(11, 3.374535174743225), (12, 3.374535174743225)],\n",
       " [(13, 3.374535174743225), (14, 3.374535174743225)],\n",
       " [(15, 3.374535174743225), (16, 3.374535174743225)],\n",
       " [(17, 3.374535174743225), (18, 3.374535174743225)],\n",
       " [(19, 3.374535174743225), (20, 3.374535174743225)],\n",
       " [(2, 0.8003672970276591), (21, 3.0068991974006543), (22, 3.0068991974006543)],\n",
       " [(8, 0.0),\n",
       "  (23, 1.8183241588185333),\n",
       "  (24, 0.1563979465125321),\n",
       "  (25, 0.1563979465125321),\n",
       "  (26, 1.8183241588185333),\n",
       "  (27, 0.1563979465125321),\n",
       "  (28, 1.8183241588185333),\n",
       "  (29, 2.805936056815044)],\n",
       " [(2, 0.5181306794428077),\n",
       "  (8, 0.0),\n",
       "  (24, 0.16742818914859228),\n",
       "  (25, 0.16742818914859228),\n",
       "  (27, 0.16742818914859228),\n",
       "  (30, 1.9465646959228444),\n",
       "  (31, 1.9465646959228444),\n",
       "  (32, 1.9465646959228444)],\n",
       " [(2, 0.5181306794428077),\n",
       "  (8, 0.0),\n",
       "  (24, 0.16742818914859228),\n",
       "  (25, 0.16742818914859228),\n",
       "  (27, 0.16742818914859228),\n",
       "  (33, 1.9465646959228444),\n",
       "  (34, 1.9465646959228444),\n",
       "  (35, 1.9465646959228444)],\n",
       " [(2, 0.5181306794428077),\n",
       "  (8, 0.0),\n",
       "  (24, 0.16742818914859228),\n",
       "  (25, 0.16742818914859228),\n",
       "  (27, 0.16742818914859228),\n",
       "  (36, 1.9465646959228444),\n",
       "  (37, 1.9465646959228444),\n",
       "  (38, 1.9465646959228444)],\n",
       " [(2, 0.48399600271660387),\n",
       "  (8, 0.0),\n",
       "  (24, 0.1563979465125321),\n",
       "  (25, 0.1563979465125321),\n",
       "  (27, 0.1563979465125321),\n",
       "  (39, 1.8183241588185333),\n",
       "  (40, 1.8183241588185333),\n",
       "  (41, 1.8183241588185333)],\n",
       " [(8, 0.0),\n",
       "  (24, 0.18013234214253698),\n",
       "  (25, 0.18013234214253698),\n",
       "  (27, 0.18013234214253698),\n",
       "  (42, 2.0942665604378328),\n",
       "  (43, 2.0942665604378328),\n",
       "  (44, 2.0942665604378328)],\n",
       " [(8, 0.0),\n",
       "  (24, 0.1563979465125321),\n",
       "  (25, 0.1563979465125321),\n",
       "  (27, 0.1563979465125321),\n",
       "  (45, 1.8183241588185333),\n",
       "  (46, 1.8183241588185333),\n",
       "  (47, 1.396692460853571),\n",
       "  (48, 1.396692460853571),\n",
       "  (49, 1.396692460853571)],\n",
       " [(8, 0.0),\n",
       "  (24, 0.1563979465125321),\n",
       "  (25, 0.1563979465125321),\n",
       "  (27, 0.1563979465125321),\n",
       "  (47, 1.396692460853571),\n",
       "  (48, 1.396692460853571),\n",
       "  (49, 1.396692460853571),\n",
       "  (50, 1.8183241588185333),\n",
       "  (51, 1.8183241588185333)]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bm25_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing similarity between documents**\n",
    "- In Gensim, SparseMatrixSimilarity is a class used to compute similarities between documents based on their vector representations in a high-dimensional space. This class is often used in conjunction with the bag-of-words (BoW) representation of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_index = SparseMatrixSimilarity(bm25_corpus, num_docs=len(corpus), num_terms=len(dictionary),\n",
    "                                   normalize_queries=False, normalize_documents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.similarities.docsim.SparseMatrixSimilarity"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bm25_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity among documents**\n",
    "- 1st vs others (including itself)\n",
    "- 2nd vs others (including itself)\n",
    "- 3rd vs others (including itself) and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.774975  0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.      ] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.         18.723473    0.          0.          0.          0.\n",
      "  0.          0.          0.          0.6405878   0.          0.41469485\n",
      "  0.41469485  0.41469485  0.38737458  0.          0.          0.        ] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.        0.       22.056652  0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.      ] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.        0.        0.       22.774975  0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.      ] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.        0.        0.        0.       22.774975  0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.      ] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.        0.        0.        0.        0.       22.774975  0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.      ] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.        0.        0.        0.        0.        0.       22.774975\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.      ] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.        0.        0.        0.        0.        0.        0.\n",
      " 22.774975  0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.      ] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.        0.        0.        0.        0.        0.        0.\n",
      "  0.       22.774975  0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.      ] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.          0.6405878   0.          0.          0.          0.\n",
      "  0.          0.          0.         18.723473    0.          0.41469485\n",
      "  0.41469485  0.41469485  0.38737458  0.          0.          0.        ] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         17.865568    0.07855628\n",
      "  0.07855628  0.07855628  0.07338096  0.08451699  0.07338096  0.07338096] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.          0.41469485  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.41469485  0.07855628 11.719898\n",
      "  0.35255602  0.35255602  0.32932943  0.0904777   0.07855628  0.07855628] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.          0.41469485  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.41469485  0.07855628  0.35255602\n",
      " 11.719898    0.35255602  0.32932943  0.0904777   0.07855628  0.07855628] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.          0.41469485  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.41469485  0.07855628  0.35255602\n",
      "  0.35255602 11.719898    0.32932943  0.0904777   0.07855628  0.07855628] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.          0.38737458  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.38737458  0.07338096  0.32932943\n",
      "  0.32932943  0.32932943 10.226542    0.08451699  0.07338096  0.07338096] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.08451699  0.0904777\n",
      "  0.0904777   0.0904777   0.08451699 13.255202    0.08451699  0.08451699] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.07338096  0.07855628\n",
      "  0.07855628  0.07855628  0.07338096  0.08451699 12.538238    5.925631  ] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.07338096  0.07855628\n",
      "  0.07855628  0.07855628  0.07338096  0.08451699  5.925631   12.538237  ] \n",
      " length=18 \n",
      " ----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in bm25_index:\n",
    "    print(i,\"\\n\",f\"length={len(i)}\",\"\\n\",\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing, cleaning and tokenizing the query string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"goblet fire\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_str = clean_string(query)\n",
    "tokenized_query = word_tokenize(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['goblet', 'fire']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The TfidfModel in Gensim is used to transform a bag-of-words (BoW) representation of documents into a Term Frequency-Inverse Document Frequency (TF-IDF) representation.**\n",
    "- Binary weighting of queries, in the context of information retrieval and text processing, refers to a representation where the presence or absence of a term in a query is considered, but not its frequency. In binary weighting, each term in the query is treated as a binary feature – either it is present in the query or it is not, without accounting for how many times it appears.\n",
    "\n",
    "- The binary representation of a query is often used in information retrieval models, including vector space models like TF-IDF. In a binary weighting scheme:\n",
    "  - If a term is present in the query, its weight is set to 1.\n",
    "  - If a term is absent in the query, its weight is set to 0.\n",
    "\n",
    "- This binary approach is in contrast to term frequency-based representations where the actual frequency of each term in the query is considered. Binary weighting simplifies the representation of queries and can be useful in scenarios where the primary concern is whether a term is present or not, rather than how many times it occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfModel(dictionary=dictionary, smartirs='bnn') # Enforce binary weighting of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(37, 1), (38, 1)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.doc2bow(tokenized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(37, 1.0), (38, 1.0)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_query = tfidf_model[dictionary.doc2bow(tokenized_query)]\n",
    "tfidf_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 3.8931293, 0.       , 0.       , 0.       , 0.       ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = bm25_index[tfidf_query]\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harry potter and the goblet of fire 2005'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_document = corpus[np.argmax(similarities)]\n",
    "\" \".join(best_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without binary weighing of queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfModel(dictionary=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(37, 1), (38, 1)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.doc2bow(tokenized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(37, 0.7071067811865476), (38, 0.7071067811865476)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_query = tfidf_model[dictionary.doc2bow(tokenized_query)]\n",
    "tfidf_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 2.7528582, 0.       , 0.       , 0.       , 0.       ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = bm25_index[tfidf_query]\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harry potter and the goblet of fire 2005'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_document = corpus[np.argmax(similarities)]\n",
    "\" \".join(best_document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
